# BA_Projekt_Local_RAG
# 

Dieses Repository enthÃ¤lt die prototypische Implementierung einer **lokalen KIâ€‘Infrastruktur** zur sicheren Nutzung von Large Language Models (LLMs) im Engineeringâ€‘Kontext.  
Der Prototyp kombiniert **semantisches Retrieval (RAG)**, **Openâ€‘Sourceâ€‘Modelle** und **rein lokale Datenverarbeitung**, um technische Fragen auf Basis interner Dokumente zu beantworten.

---

##  **Funktionen**
- VollstÃ¤ndig **Onâ€‘Premise** (keine Cloud, volle Datenhoheit)  
- **RAGâ€‘Pipeline**: semantische Suche + generative Antwort  
- Nutzung von **LLaMA 3B (4â€‘bit)** via Ollama  
- **ChromaDB** als Vektorâ€‘Datenbank  
- **Streamlit Webâ€‘UI** fÃ¼r einfache Bedienung  
- Antworten **inkl. Quellenpassagen**

---

##  **Technologien**
- **Python**, **LangChain**
- **LLaMA 3B**, **mxbaiâ€‘embedâ€‘large**
- **ChromaDB**
- **Streamlit**
- GPU empfohlen (z.â€¯B. RTX 3050 Ti)

---

##  **Struktur**
```
app.py               # Entry Point
config.py            # Konfiguration
models/              # LLM + Embeddings
pipeline/            # Indexing + RAG
database/            # Vector Store
ui/                  # Streamlit-UI
data/pdfs/           # Dokumente
```

---

##  **Schnellstart**

### 1. Modelle installieren
```bash
ollama pull llama3:3b
ollama pull mxbai-embed-large
```

### 2. AbhÃ¤ngigkeiten
```bash
pip install -r requirements.txt
```

### 3. Index bauen
```bash
python pipeline/indexing.py
```

### 4. Web-App starten
```bash
streamlit run ui/streamlit_app.py
```

---

##  **Limitierungen**
- Kleines Modell â†’ begrenztes Reasoning  
- Kein RBAC, Monitoring oder DMSâ€‘Integration  
- Prototyp, nicht produktionsbereit  

---

## ðŸ‘¤ **Autor**
**Azmi Zaafouri** â€“ TH KÃ¶ln  
Bachelorarbeit: *â€žLokale KIâ€‘Infrastruktur fÃ¼r sensible Engineeringâ€‘Daten: Ein Konzept fÃ¼r mittelstÃ¤ndische Unternehmenâ€œ*
